{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup + Mount Drive"
      ],
      "metadata": {
        "id": "FtD-eY3gGGLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T0BjwucF9J_",
        "outputId": "9d318cbb-4d36-4b1a-ff28-45aad7d05510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-_Xz0OCRH_Z",
        "outputId": "1259924d-9e63-491b-8aaf-dd2db6050737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = Path(\"/content/drive/MyDrive/Data\")\n",
        "FEATURES_DIR = BASE / \"features\"\n",
        "TEXT_DIR = BASE / \"text_features_bert\"\n",
        "MODEL_DIR = BASE / \"models\"\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"FEATURES_DIR exists:\", FEATURES_DIR.exists())\n",
        "print(\"TEXT_DIR exists:\", TEXT_DIR.exists())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amh6EEojRJlZ",
        "outputId": "bbd84986-685c-45f1-f33f-e2ef40053bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FEATURES_DIR exists: True\n",
            "TEXT_DIR exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Spatial Features (ResNet-50 7Ã—7)"
      ],
      "metadata": {
        "id": "AXO_gQy6RQXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spatial_path = FEATURES_DIR / \"features_spatial.pt\"\n",
        "\n",
        "if not spatial_path.exists():\n",
        "    raise FileNotFoundError(\"features_spatial.pt not found in features folder\")\n",
        "\n",
        "features_spatial = torch.load(str(spatial_path), map_location=\"cpu\")\n",
        "\n",
        "print(\"Loaded spatial features:\", len(features_spatial))\n",
        "sample_key = list(features_spatial.keys())[0]\n",
        "print(\"Example tensor shape:\", features_spatial[sample_key].shape)  # (2048,7,7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ovqWAtROUJ",
        "outputId": "1cbde843-15fd-40e2-8e09-7cccafcf3246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded spatial features: 1449\n",
            "Example tensor shape: torch.Size([2048, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Text Features (BERT pooled)"
      ],
      "metadata": {
        "id": "q0TplhzjRUA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_all = np.load(str(TEXT_DIR / \"q_bert_pooled.npy\"))\n",
        "ans_all = np.load(str(TEXT_DIR / \"answer_idx.npy\"))\n",
        "\n",
        "print(\"q_all shape:\", q_all.shape)\n",
        "print(\"ans_all shape:\", ans_all.shape)\n",
        "\n",
        "answer2idx_path = TEXT_DIR / \"answer2idx.json\"\n",
        "answer2idx = json.load(open(answer2idx_path)) if answer2idx_path.exists() else None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ymbkeTwRS-Z",
        "outputId": "b993bc33-1836-4355-bdf9-c9866d49adcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_all shape: (12468, 768)\n",
            "ans_all shape: (12468,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Align With Train/Eval CSV"
      ],
      "metadata": {
        "id": "0DOVp3cwRXSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(BASE / \"data_train.csv\")\n",
        "eval_df = pd.read_csv(BASE / \"data_eval.csv\")\n",
        "combined_df = pd.read_csv(BASE / \"data.csv\")"
      ],
      "metadata": {
        "id": "_SjiUrexRWGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x): return str(x).strip()"
      ],
      "metadata": {
        "id": "eCLCMYEtRbEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = [(norm(r['question']), norm(r['answer']), norm(r['image_id']))\n",
        "        for _, r in combined_df.iterrows()]\n",
        "\n",
        "mapping = defaultdict(list)\n",
        "for i,k in enumerate(keys):\n",
        "    mapping[k].append(i)"
      ],
      "metadata": {
        "id": "sS-BQFM_Rclp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_df(df):\n",
        "    idxs=[]\n",
        "    for _,r in df.iterrows():\n",
        "        k=(norm(r['question']), norm(r['answer']), norm(r['image_id']))\n",
        "        if mapping[k]:\n",
        "            idxs.append(mapping[k].pop(0))\n",
        "        else:\n",
        "            idxs.append(None)\n",
        "    return idxs"
      ],
      "metadata": {
        "id": "j_RBpi1kRe4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = map_df(train_df)\n",
        "eval_idx  = map_df(eval_df)"
      ],
      "metadata": {
        "id": "BAeVol17Rgo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_subset(idxs):\n",
        "    qs=[]; labs=[]\n",
        "    for i in idxs:\n",
        "        if i is None:\n",
        "            qs.append(np.zeros(q_all.shape[1], dtype=np.float32))\n",
        "            labs.append(-1)\n",
        "        else:\n",
        "            qs.append(q_all[i])\n",
        "            labs.append(int(ans_all[i]))\n",
        "    return np.stack(qs), np.array(labs)"
      ],
      "metadata": {
        "id": "xtKl-K8CRjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_train, ans_train = build_subset(train_idx)\n",
        "q_eval, ans_eval   = build_subset(eval_idx)\n",
        "\n",
        "print(\"Train:\", q_train.shape, ans_train.shape)\n",
        "print(\"Eval :\", q_eval.shape, ans_eval.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfFSV6ELRmQC",
        "outputId": "b505ef64-b6a9-47fd-e9e3-fef3258b9a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (6795, 768) (6795,)\n",
            "Eval : (5673, 768) (5673,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset (Spatial + Question)"
      ],
      "metadata": {
        "id": "64dd5FbNRowR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQAAttentionDataset(Dataset):\n",
        "    def __init__(self, df, q_feats, labels):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.q = q_feats\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.q)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = str(self.df.loc[idx, \"image_id\"]).strip()\n",
        "\n",
        "        if img_id in features_spatial:\n",
        "            img_feat = features_spatial[img_id]  # (2048,7,7)\n",
        "        else:\n",
        "            img_feat = torch.zeros(2048,7,7)\n",
        "\n",
        "        return (\n",
        "            img_feat.float(),\n",
        "            torch.tensor(self.q[idx], dtype=torch.float32),\n",
        "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "train_dataset = VQAAttentionDataset(train_df, q_train, ans_train)\n",
        "eval_dataset  = VQAAttentionDataset(eval_df,  q_eval,  ans_eval)\n"
      ],
      "metadata": {
        "id": "yAMd-PoNRnNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention VQA Model"
      ],
      "metadata": {
        "id": "NXAF5ffHTeNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_label = max(ans_train.max(), ans_eval.max())\n",
        "num_classes = int(max_label) + 1\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "class AttentionVQA(nn.Module):\n",
        "    def __init__(self, img_dim=2048, txt_dim=768, hidden=512, num_classes=583):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_proj = nn.Linear(img_dim, hidden)\n",
        "        self.txt_proj = nn.Linear(txt_dim, hidden)\n",
        "\n",
        "        self.attention = nn.Linear(hidden, 1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden*2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, txt):\n",
        "        B = img.size(0)\n",
        "\n",
        "        img = img.view(B, 2048, -1).permute(0,2,1)  # (B,49,2048)\n",
        "        img_feat = self.img_proj(img)              # (B,49,512)\n",
        "\n",
        "        txt_feat = self.txt_proj(txt).unsqueeze(1) # (B,1,512)\n",
        "\n",
        "        joint = torch.tanh(img_feat + txt_feat)\n",
        "\n",
        "        attn_scores = self.attention(joint).squeeze(-1)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "\n",
        "        attended = torch.sum(img_feat * attn_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        fused = torch.cat([attended, txt_feat.squeeze(1)], dim=1)\n",
        "\n",
        "        return self.classifier(fused)\n",
        "\n",
        "model = AttentionVQA(\n",
        "    img_dim=2048,\n",
        "    txt_dim=768,\n",
        "    hidden=512,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "print(\"Model ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP6-gczcTeis",
        "outputId": "1f658469-9c46-4a5b-92af-44cdf6bbd1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 583\n",
            "Model ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "Dh9T2YggTiDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "eval_loader  = DataLoader(eval_dataset, batch_size=32)\n",
        "\n",
        "epochs = 12\n",
        "best_val = 0\n",
        "\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    total_loss=0\n",
        "\n",
        "    for img,q,l in train_loader:\n",
        "        img,q,l = img.to(device), q.to(device), l.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(img,q)\n",
        "        loss = criterion(out,l)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    correct=0; total=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img,q,l in eval_loader:\n",
        "            img,q,l = img.to(device), q.to(device), l.to(device)\n",
        "            out = model(img,q)\n",
        "            preds = out.argmax(1)\n",
        "\n",
        "            mask = l!=-1\n",
        "            correct += (preds[mask]==l[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "\n",
        "    acc = correct/total\n",
        "    print(f\"Epoch {ep+1} Loss {total_loss:.2f} Val Acc {acc:.4f}\")\n",
        "\n",
        "    if acc>best_val:\n",
        "        best_val=acc\n",
        "        torch.save(model.state_dict(), MODEL_DIR/\"attention_best.pth\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm2QpQahThDK",
        "outputId": "d17632d1-008a-4789-f2de-d2db42a1b199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 1081.63 Val Acc 0.1075\n",
            "Epoch 2 Loss 988.94 Val Acc 0.1206\n",
            "Epoch 3 Loss 935.38 Val Acc 0.1248\n",
            "Epoch 4 Loss 879.83 Val Acc 0.1585\n",
            "Epoch 5 Loss 835.65 Val Acc 0.1585\n",
            "Epoch 6 Loss 798.08 Val Acc 0.1666\n",
            "Epoch 7 Loss 761.31 Val Acc 0.1641\n",
            "Epoch 8 Loss 728.27 Val Acc 0.1756\n",
            "Epoch 9 Loss 694.29 Val Acc 0.1868\n",
            "Epoch 10 Loss 663.15 Val Acc 0.1890\n",
            "Epoch 11 Loss 632.96 Val Acc 0.1831\n",
            "Epoch 12 Loss 606.27 Val Acc 0.1868\n",
            "Training complete.\n"
          ]
        }
      ]
    }
  ]
}